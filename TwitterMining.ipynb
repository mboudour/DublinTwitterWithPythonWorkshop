{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining (together with a bit of web scraping) of large social networks from Twitter using Python (and Ruby)\n",
    "## By Moses Boudourides and Sergios Lenis \n",
    "## University of Patras, Greece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I. Prerequisite Python Modules and Scripts](#I)\n",
    "\n",
    "[II. Twitter Mining from the Twitter API](#II)\n",
    "\n",
    "[III. Web Scraping from the Twitter Advanced Search](#III)\n",
    "\n",
    "[IV. Visualization of Timeseries, Geolocations & Network Analysis of Twitter Data](#IV)\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='I'></a>\n",
    "## I. Prerequisite Python Modules and Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The following cell imports the prerequisite Python modules for this network to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First, one has to download the *github* directory *https://github.com/mboudour/TwitterMining*, where everything needed for this notebook to run is included.**\n",
    "\n",
    "### **Github Blog: *http://mboudour.github.io/***\n",
    "\n",
    "### **Furthermore, one needs to have already installed all the modules imported in the script** *collect_tweets_notebook.py*. Some of these modules can be installed and imported from the notebook as follows (without #):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-twitter\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /Users/mosesboudourides/anaconda/lib/python2.7/site-packages (from python-twitter)\n",
      "Collecting future (from python-twitter)\n",
      "Collecting requests-oauthlib (from python-twitter)\n",
      "  Downloading requests_oauthlib-0.6.2-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=0.6.2 (from requests-oauthlib->python-twitter)\n",
      "Installing collected packages: future, oauthlib, requests-oauthlib, python-twitter\n",
      "Successfully installed future-0.15.2 oauthlib-1.1.2 python-twitter-3.1 requests-oauthlib-0.6.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install python-twitter\n",
    "# import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='II'></a>\n",
    "## II. Twitter Mining from the Twitter API (https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting Input and Output Directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir='/home/mosesboudourides/Dropbox/Python Projects/DublinNovemer2016'\n",
    "\n",
    "output_dir='/home/mosesboudourides/Dropbox/Python Projects/DublinNovemer2016/Out_json'\n",
    "\n",
    "# cred_dic=None # Run this only the first time!\n",
    "cred_dic='/home/mosesboudourides/Dropbox/Python Projects/DublinNovemer2016/credentials/auth_cred.txt'\n",
    "\n",
    "pp= !pwd # for Mac OS X or Unix\n",
    "# use 'pp = !cd' in Windows\n",
    "\n",
    "os.chdir(input_dir)\n",
    "from test_class_tpa import create_df\n",
    "import collect_tweets_notebook as ctn\n",
    "\n",
    "os.chdir(pp[0])\n",
    "\n",
    "def create_beaker_com_dict(sps):\n",
    "    nsps={}\n",
    "    for k,v in sps.items():\n",
    "        nsps[k]=[]\n",
    "        if k=='date_split':\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk].strftime('%Y%m%d'))\n",
    "        else:\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk])\n",
    "\n",
    "    return nsps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authentication and login in Twitter API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv=ctn.UserAuth(auth_file=cred_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **After the authentication tokens are known, one has to insert them below by decommenting and running the following three cells:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vv.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv.check_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twi_api=vv.get_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up a Search**\n",
    "#### **Further info about how to build a Twitter query is available at: https://dev.twitter.com/rest/public/search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_term='Charlie Flanagan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sea=ctn.TwitterSearch(twi_api,search_text=search_term,working_path=output_dir,out_file_dir=None,\n",
    "max_pages=10,results_per_page=100,sin_id=None,max_id=None,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sea.streamsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>To interrupt the collection of tweets initiated above, one has to click \"Kernel > Interrupt\" from the Notebook menu.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The data collected from the above search are saved as a json file in the above defined output_dir named by the above defined search_term.**\n",
    "\n",
    "### **In the json file, there are four main “objects” provided by the API:** \n",
    "* **Tweets,** \n",
    "* **Users,** \n",
    "* **Entities and** \n",
    "* **Places.**\n",
    "\n",
    "### **Definitions and info about all these ojects is given in https://dev.twitter.com/overview/api.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our purposes, we are selecting 21 practically intersesting \"objects\" and eventually creating a Pandas data frame with them as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter json \"objects\" https://dev.twitter.com/rest/reference/get/search/tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "user_id\n",
      "username\n",
      "created_at\n",
      "language\n",
      "hashtag_count\n",
      "retweet_count\n",
      "mention_count\n",
      "statuses_count\n",
      "followers_count\n",
      "friends_count\n",
      "listed_count\n",
      "videos_count\n",
      "photos_count\n",
      "undef_count\n",
      "coordinates\n",
      "bounding\n",
      "place\n",
      "hashtags\n",
      "mentions\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "columnss=['id','user_id','username','created_at','language','hashtag_count','retweet_count','mention_count',\n",
    "          'statuses_count','followers_count','friends_count','listed_count','videos_count','photos_count',\n",
    "          'undef_count','coordinates','bounding','place','hashtags','mentions','text'] \n",
    "for i in columnss:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='III'></a>\n",
    "## III. Web Scraping from the Twitter Advanced Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since this is implemented in *Ruby*, it is required that this programming language is already installed in the computer where one intends to run the following procedure.\n",
    "\n",
    "### **The Twitter advanced search page is located at *https://twitter.com/search-advanced.* **\n",
    "\n",
    "### **For Twitter Scraping, one should run the following two Ruby scripts:\n",
    "1. scraping_script.rb\n",
    "2. getting_json_script.rb\n",
    "\n",
    "### **If the searched term is a hashtag (or multiple hashtags), one should insert it (them) directly in line 22 after 'searchterm=' in the script *scraping_script.rb*. One should also insert the dates *from* and *to* in lines 23 and 26 of this script. Moreover, line 29 should contain \"hashtag=true\".**\n",
    "\n",
    "### **For non-hashtag-type search terms, one has to open the page with the outcome of the Twitter search, copy the substring in the URL that follows 'search?q=' before '&src=\" and paste it in line 22 after 'searchterm=' in the script *scraping_script.rb*. Again, one should also insert the dates *from* and *to* in lines 23 and 26 of this script. Of course now, line 29 should contain \"hashtag=false\".**\n",
    "\n",
    "### **Subsequently, one should run the script *getting_json_script.rb* to generate the json file of the scraped tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='IV'></a>\n",
    "## IV. Visualization of Timeseries, Geolocations & Network Analysis of Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is done using (the template of) the *Beaker Notebook*:**\n",
    "* beaker_analysis_base_Viz.bkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
