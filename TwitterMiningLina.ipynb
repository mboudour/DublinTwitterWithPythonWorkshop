{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining (together with a bit of web scraping) of large social networks from Twitter using Python (and Ruby)\n",
    "## By Moses Boudourides and Sergios Lenis \n",
    "## University of Patras, Greece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "[I. Prerequisite Python Modules and Scripts](#I)\n",
    "\n",
    "[II. Twitter Mining from the Twitter API](#II)\n",
    "\n",
    "[III. Web Scraping from the Twitter Advanced Search](#III)\n",
    "\n",
    "[IV. Visualization of Timeseries, Geolocations & Network Analysis of Twitter Data](#IV)\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>\n",
    "\n",
    "<p> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='I'></a>\n",
    "## I. Prerequisite Python Modules and Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The following cell imports the prerequisite Python modules for this network to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json \n",
    "import os\n",
    "import imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First, one has to download the *github* directory *https://github.com/mboudour/DublinTwitterWithPythonWorkshop*, where everything needed for this notebook to run is included.**\n",
    "\n",
    "### **Furthermore, one needs to have already installed all the modules imported in the script** *collect_tweets_notebook.py*. Some of these modules can be installed and imported from the notebook through the *pip* command as follows:**\n",
    "\n",
    "pip install --user module_name\n",
    "\n",
    "### **For instance:**\n",
    "\n",
    "pip install --user python--twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --user python--twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "#Αν δεν εκτελεστεί, κάνουμε restart από το kernel\n",
    "#και μετά ξανατρέχουμε τα κελιά 1 και 3.\n",
    "#(όχι το κελί !pip install --user python--twitter\n",
    "#το οποίο κελί για τη συνέχεια το αδρανοποιούμε βάζοντας # στην αρχή) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='II'></a>\n",
    "## II. Twitter Mining from the Twitter API (https://apps.twitter.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting Input and Output Directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am being imported from another module\n"
     ]
    }
   ],
   "source": [
    "# Set the pwd's for your computer:\n",
    "\n",
    "# 1. Ορίζουμε το input_dir όπου βρίσκονται τα script του github.\n",
    "input_dir='C:\\Users\\Lina\\Desktop\\Dublin Twitter\\DublinTwitterWithPythonWorkshop-master'\n",
    "\n",
    "# 2. Ορίζουμε το output_dir. Φτιάχνουμε έναν φάκελο ο οποίος θα λέγεται \n",
    "# \"Out_json\" στην ίδια θέση με τα αρχεία του github.\n",
    "output_dir='C:\\Users\\Lina\\Desktop\\Dublin Twitter\\DublinTwitterWithPythonWorkshop-master\\Out_dir'\n",
    "\n",
    "\n",
    "# 3. Την πρώτη φορά κρατούμε ανοικτή (χωρίς #)την εντολή \"cred_dic=None\"\n",
    "# έτσι, δημιουργείται ο φάκελος cred_dic στον οποίο αποθηκεύονται\n",
    "# οι κωδικοί του twitter.\n",
    "# Τις επόμενες φορές κρατούμε κλειστή την εντολή cred_dic=None και \n",
    "# προσθέτουμε το cred_dic=\n",
    "\n",
    "# Run \"cred_dic=None\" only the first time! Subsequently, a cred_dir is\n",
    "# created and you need to write its pwd in the line below.\n",
    "\n",
    "# cred_dic=None \n",
    "# cred_dic='/home/mosesboudourides/Dropbox/Python Projects/DublinNovemer2016/credentials/auth_cred.txt'\n",
    "\n",
    "# pp= !pwd # for Mac OS X or Unix\n",
    "pp = !cd \n",
    "\n",
    "os.chdir(input_dir)\n",
    "from test_class_tpa import create_df\n",
    "import collect_tweets_notebook as ctn\n",
    "\n",
    "os.chdir(pp[0])\n",
    "\n",
    "def create_beaker_com_dict(sps):\n",
    "    nsps={}\n",
    "    for k,v in sps.items():\n",
    "        nsps[k]=[]\n",
    "        if k=='date_split':\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk].strftime('%Y%m%d'))\n",
    "        else:\n",
    "            for kk in sorted(v.keys()):\n",
    "                nsps[k].append(v[kk])\n",
    "\n",
    "    return nsps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authentication and login in Twitter API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vv=ctn.UserAuth()#(auth_file=cred_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **After the authentication tokens are known, one has to insert them below by decommenting and running the following three cells:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to http://twitter.com/apps/new to create an app and get these items.\n",
      "Give me the Consumer key: n7ngqIfaOBNbjCLlzOpGWEKQ2\n",
      "Give me the Consumer secret: qGYaVJrXLe4kNYdf1OmH2YDKGvH3KZgEGplHniYdMre8LttpWq\n",
      "Give me the Access token: 803991097173229568-zCxvAyj0iJEt2OTYX3O4DSyyLpmOrtJ\n",
      "Give me the Access token secret: DcsybbuvepRG1whPPpcIXRNiWHZBXMqQoaifsZWM7W9Jm\n"
     ]
    }
   ],
   "source": [
    "vv.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vv.check_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twi_api=vv.get_auth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setting up a Search**\n",
    "#### **Further info about how to build a Twitter query is available at: https://dev.twitter.com/rest/public/search.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_term='fhollande'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sea=ctn.TwitterSearch(twi_api,search_text=search_term,working_path=output_dir,out_file_dir=None,\n",
    "max_pages=10,results_per_page=100,sin_id=None,max_id=None,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sea.streamsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font color='red'>To interrupt the collection of tweets initiated above, one has to click \"Kernel > Interrupt\" from the Notebook menu.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The data collected from the above search are saved as a json file in the above defined output_dir named by the above defined search_term.**\n",
    "\n",
    "### **In the json file, there are four main “objects” provided by the API:** \n",
    "* **Tweets,** \n",
    "* **Users,** \n",
    "* **Entities and** \n",
    "* **Places.**\n",
    "\n",
    "### **Definitions and info about all these ojects is given in https://dev.twitter.com/overview/api.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our purposes, we are selecting 21 practically intersesting \"objects\" and eventually creating a Pandas data frame with them as columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter json \"objects\" https://dev.twitter.com/rest/reference/get/search/tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "user_id\n",
      "username\n",
      "created_at\n",
      "language\n",
      "hashtag_count\n",
      "retweet_count\n",
      "mention_count\n",
      "statuses_count\n",
      "followers_count\n",
      "friends_count\n",
      "listed_count\n",
      "videos_count\n",
      "photos_count\n",
      "undef_count\n",
      "coordinates\n",
      "bounding\n",
      "place\n",
      "hashtags\n",
      "mentions\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "columnss=['id','user_id','username','created_at','language','hashtag_count','retweet_count','mention_count',\n",
    "          'statuses_count','followers_count','friends_count','listed_count','videos_count','photos_count',\n",
    "          'undef_count','coordinates','bounding','place','hashtags','mentions','text'] \n",
    "for i in columnss:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='III'></a>\n",
    "## III. Web Scraping from the Twitter Advanced Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since this is implemented in *Ruby*, it is required that this programming language is already installed in the computer where one intends to run the following procedure.\n",
    "\n",
    "### **The Twitter advanced search page is located at *https://twitter.com/search-advanced.* **\n",
    "\n",
    "### **For Twitter Scraping, one should run the following two Ruby scripts:\n",
    "1. scraping_script.rb\n",
    "2. getting_json_script.rb\n",
    "\n",
    "### **If the searched term is a hashtag (or multiple hashtags), one should insert it (them) directly in line 22 after 'searchterm=' in the script *scraping_script.rb*. One should also insert the dates *from* and *to* in lines 23 and 26 of this script. Moreover, line 29 should contain \"hashtag=true\".**\n",
    "\n",
    "### **For non-hashtag-type search terms, one has to open the page with the outcome of the Twitter search, copy the substring in the URL that follows 'search?q=' before '&src=\" and paste it in line 22 after 'searchterm=' in the script *scraping_script.rb*. Again, one should also insert the dates *from* and *to* in lines 23 and 26 of this script. Of course now, line 29 should contain \"hashtag=false\".**\n",
    "\n",
    "### **Subsequently, one should run the script *getting_json_script.rb* to generate the json file of the scraped tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='IV'></a>\n",
    "## IV. Visualization of Timeseries, Geolocations & Network Analysis of Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **This is done using (the template of) the *Beaker Notebook*:**\n",
    "* beaker_analysis_base_Viz.bkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
